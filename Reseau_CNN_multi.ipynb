{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4x_JO6sZVSQ"
      },
      "source": [
        "# **Analyse du DataSet**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNJTQEF6Zf5j"
      },
      "source": [
        "## **Télécharger les données**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yrgzuSnMZqJj",
        "outputId": "f2d4f956-0ccf-49de-85c0-5d70e2805255"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vg7kjFz9Zo-y"
      },
      "outputs": [],
      "source": [
        "path_to_dataset = 'dataset_equilibre'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uQTQv0OmboH9"
      },
      "outputs": [],
      "source": [
        "def set_category(path_to_dataset):\n",
        "  return os.listdir(path_to_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ld5U1hFsanfS"
      },
      "outputs": [],
      "source": [
        "def load_category(path_to_dataset,category):\n",
        "  return f\"{path_to_dataset}/{category}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPpQ7mH7baoR",
        "outputId": "2400de26-e9f6-4e43-c187-a8f0a88e698b"
      },
      "outputs": [],
      "source": [
        "categories = set_category(path_to_dataset)\n",
        "path_to_categories = [load_category(path_to_dataset, category) for category in categories]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoEL_iUNgRfK",
        "outputId": "ffc1cf95-0d95-4895-f8f6-3167aba98622"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes disponibles : ['Academic_Art', 'Art_Nouveau', 'Baroque', 'Expressionism', 'Japanese_Art', 'Neoclassicism', 'Primitivism', 'Realism', 'Renaissance', 'Rococo', 'Romanticism', 'Symbolism', 'Western_Medieval']\n",
            "Nombre total d'images : 7800\n"
          ]
        }
      ],
      "source": [
        "# Paramètres globaux\n",
        "IMG_SIZE = (128, 128)\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Fonction pour charger et prétraiter une image\n",
        "def load_image(img_path):\n",
        "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=IMG_SIZE)\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = img_array / 255.0  # Normalisation\n",
        "    return img_array\n",
        "\n",
        "# Préparer les chemins et les labels\n",
        "def prepare_data(path_to_categories):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    class_names = [os.path.basename(cat_path) for cat_path in path_to_categories]\n",
        "\n",
        "    for label, category_path in enumerate(path_to_categories):\n",
        "        images = [os.path.join(category_path, img) for img in os.listdir(category_path) if img.endswith(('png', 'jpg', 'jpeg'))]\n",
        "        image_paths.extend(images)\n",
        "        labels.extend([label] * len(images))\n",
        "\n",
        "    return image_paths, labels, class_names\n",
        "\n",
        "image_paths, labels, class_names = prepare_data(path_to_categories)\n",
        "print(f\"Classes disponibles : {class_names}\")\n",
        "print(f\"Nombre total d'images : {len(image_paths)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPVMPEwRk9G2"
      },
      "source": [
        "## **DataGenerator création**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Je5C4j9tgn4S"
      },
      "outputs": [],
      "source": [
        "# Création d'un data generator personnalisé avec super().__init__()\n",
        "class CustomDataGenerator(Sequence):\n",
        "    def __init__(self, image_paths, labels, batch_size, shuffle=True, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        start = index * self.batch_size\n",
        "        end = (index + 1) * self.batch_size\n",
        "        batch_image_paths = self.image_paths[start:end]\n",
        "        batch_labels = self.labels[start:end]\n",
        "\n",
        "        images = np.array([load_image(img_path) for img_path in batch_image_paths])\n",
        "        labels = tf.keras.utils.to_categorical(batch_labels, num_classes=len(set(self.labels)))\n",
        "        return images, labels\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            temp = list(zip(self.image_paths, self.labels))\n",
        "            np.random.shuffle(temp)\n",
        "            self.image_paths, self.labels = zip(*temp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        },
        "id": "x9Mlg1G2gn3P",
        "outputId": "af58d34f-d753-4cd8-98f0-9e34f03d604c"
      },
      "outputs": [],
      "source": [
        "# Séparer les données en ensembles d'entraînement et de validation\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    image_paths, labels, test_size=0.2, random_state=123, stratify=labels\n",
        ")\n",
        "\n",
        "# Instancier les générateurs\n",
        "generator_train = CustomDataGenerator(train_paths, train_labels, BATCH_SIZE)\n",
        "generator_val = CustomDataGenerator(val_paths, val_labels, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m82v47KhiF5"
      },
      "source": [
        "# **Création du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZIHbbYpjDCb",
        "outputId": "ada48faa-1e4d-4fc8-e095-5f3d76a82378"
      },
      "outputs": [],
      "source": [
        "# Generate hyperparameter combinations\n",
        "def generate_hyperparameter_combinations(learning_rates):\n",
        "    dropout_options = [0.2, 0.4, 0.5]\n",
        "    batch_sizes = [16]\n",
        "    epochs = [5, 6]\n",
        "    batch_norm_options = [True]\n",
        "    combinations = list(itertools.product(learning_rates, dropout_options, batch_sizes, epochs, batch_norm_options))\n",
        "    return [\n",
        "        {\n",
        "            \"learning_rate\": lr,\n",
        "            \"dropout\": do,\n",
        "            \"batch_size\": bs,\n",
        "            \"epochs\": ep,\n",
        "            \"batch_norm\": bn\n",
        "        }\n",
        "        for lr, do, bs, ep, bn in combinations\n",
        "    ]\n",
        "\n",
        "# Define learning rates to test\n",
        "learning_rates = [0.001, 0.0005, 0.0001]\n",
        "hyperparameter_grid = generate_hyperparameter_combinations(learning_rates)\n",
        "\n",
        "# Placeholder for results\n",
        "results = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0PmtYgJImW3q"
      },
      "outputs": [],
      "source": [
        "# Build and train models\n",
        "def build_cnn(filters, dropout, learning_rate, batch_norm, input_shape, num_classes):\n",
        "    model = Sequential()\n",
        "    for filter_size in filters:\n",
        "        model.add(Conv2D(filter_size, (3, 3), activation='relu', input_shape=input_shape))\n",
        "        if batch_norm:\n",
        "            model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EimRMax5mdT-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sober\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m130/390\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:00\u001b[0m 1s/step - accuracy: 0.1265 - loss: 35.3722"
          ]
        }
      ],
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "for config in hyperparameter_grid:\n",
        "    filters = [64]\n",
        "    dropout = config[\"dropout\"]\n",
        "    learning_rate = config[\"learning_rate\"]\n",
        "    batch_size = config[\"batch_size\"]\n",
        "    epochs = config[\"epochs\"]\n",
        "    batch_norm = config[\"batch_norm\"]\n",
        "\n",
        "    model = build_cnn(filters, dropout, learning_rate, batch_norm, (IMG_SIZE[0], IMG_SIZE[1], 3), len(class_names))\n",
        "\n",
        "    history = model.fit(\n",
        "        generator_train,\n",
        "        validation_data=generator_val,\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=len(generator_train),\n",
        "        validation_steps=len(generator_val),\n",
        "        batch_size=batch_size,\n",
        "        verbose=1,\n",
        "        callbacks=[early_stopping]\n",
        "    )\n",
        "\n",
        "    results.append({\n",
        "        \"filters\": filters,\n",
        "        \"dropout\": dropout,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs,\n",
        "        \"batch_norm\": batch_norm,\n",
        "        \"val_loss\": history.history['val_loss'],\n",
        "        \"val_accuracy\": history.history['val_accuracy']\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display results\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot evolution of performance by hyperparameters\n",
        "plt.figure(figsize=(20, 15))\n",
        "\n",
        "# Evolution of performance by epoch\n",
        "plt.subplot(3, 1, 1)\n",
        "for i, config in enumerate(hyperparameter_grid):\n",
        "    val_loss = results[i]['val_loss']\n",
        "    val_accuracy = results[i]['val_accuracy']\n",
        "    epochs = range(1, len(val_loss) + 1)\n",
        "\n",
        "    plt.plot(epochs, val_loss, marker='o', label=f'Config {i+1}: Epochs={config[\"epochs\"]}')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation Loss')\n",
        "plt.title('Validation Loss by Epochs')\n",
        "plt.legend()\n",
        "\n",
        "# Performance by dropout rate\n",
        "plt.subplot(3, 1, 2)\n",
        "for i, config in enumerate(hyperparameter_grid):\n",
        "    plt.scatter(config['dropout'], results[i]['val_accuracy'][-1], label=f'Config {i+1}')\n",
        "plt.xlabel('Dropout Rate')\n",
        "plt.ylabel('Final Validation Accuracy')\n",
        "plt.title('Final Validation Accuracy by Dropout Rate')\n",
        "plt.legend()\n",
        "\n",
        "# Performance by learning rate\n",
        "plt.subplot(3, 1, 3)\n",
        "for i, config in enumerate(hyperparameter_grid):\n",
        "    plt.scatter(config['learning_rate'], results[i]['val_accuracy'][-1], label=f'Config {i+1}')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.ylabel('Final Validation Accuracy')\n",
        "plt.title('Final Validation Accuracy by Learning Rate')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
